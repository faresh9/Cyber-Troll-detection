{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the Twitter data from CSV\"\"\"\n",
    "    df = pd.read_csv('data/data.csv', encoding='utf-8')\n",
    "    # Keep only English tweets\n",
    "    df = df[df['Language'] == 'en']\n",
    "    # Reset index after filtering\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess tweet text\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Replace emojis with their descriptions\n",
    "        text = emoji.demojize(text)\n",
    "        \n",
    "        # Remove mentions (@username)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Extract hashtags but remove the # symbol\n",
    "        hashtags = re.findall(r'#(\\w+)', text)\n",
    "        # Remove the hashtags from the text\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"Extract additional features from the data\"\"\"\n",
    "    # Text length\n",
    "    df['text_length'] = df['Tweet_Content'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)\n",
    "    \n",
    "    # Count of hashtags\n",
    "    df['hashtag_count'] = df['Tweet_Content'].apply(\n",
    "        lambda x: len(re.findall(r'#\\w+', str(x))) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Count of mentions\n",
    "    df['mention_count'] = df['Tweet_Content'].apply(\n",
    "        lambda x: len(re.findall(r'@\\w+', str(x))) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Uppercase ratio (possible indicator of shouting/aggression)\n",
    "    df['uppercase_ratio'] = df['Tweet_Content'].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper())/len(str(x)) if isinstance(x, str) and len(str(x)) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Check for controversial keywords (customize this list based on your domain)\n",
    "    troll_keywords = ['terrorist', 'nazi', 'evil', 'kill', 'murder', 'destroy', 'genocide', 'apartheid']\n",
    "    df['controversial_keyword_count'] = df['Tweet_Content'].apply(\n",
    "        lambda x: sum(str(x).lower().count(word) for word in troll_keywords) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Engagement features that may indicate viral/controversial content\n",
    "    df['engagement_score'] = df['Reply_Count'] + df['Repost_Count'] + df['Like_Count']\n",
    "    \n",
    "    # Verified status as a numeric feature\n",
    "    df['is_verified'] = df['Verified_Status'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_troll_labels(df):\n",
    "    \"\"\"\n",
    "    Create labels for trolling detection (this is a simplified heuristic approach)\n",
    "    You would ideally have human-labeled data or a more sophisticated approach\n",
    "    \"\"\"\n",
    "    # This is a simplified labeling approach - in reality you'd want human labeling or better heuristics\n",
    "    # Criteria: high controversial keywords, high uppercase ratio, and relatively high engagement\n",
    "    \n",
    "    # Normalize the features for scoring\n",
    "    df['norm_controversial'] = df['controversial_keyword_count'] / df['controversial_keyword_count'].max()\n",
    "    df['norm_uppercase'] = df['uppercase_ratio'] / df['uppercase_ratio'].max() \n",
    "    df['norm_engagement'] = np.log1p(df['engagement_score']) / np.log1p(df['engagement_score']).max()\n",
    "    \n",
    "    # Create a simple troll score\n",
    "    df['troll_score'] = (df['norm_controversial'] * 0.5 + \n",
    "                         df['norm_uppercase'] * 0.3 + \n",
    "                         df['norm_engagement'] * 0.2)\n",
    "    \n",
    "    # Label as potential troll if score is above threshold\n",
    "    # This threshold would need tuning\n",
    "    threshold = df['troll_score'].quantile(0.75)  # Top 25% as trolls for balanced dataset\n",
    "    df['troll_label'] = (df['troll_score'] > threshold).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    \"\"\"Prepare the final dataset for the model\"\"\"\n",
    "    # Clean the tweet content\n",
    "    df['cleaned_text'] = df['Tweet_Content'].apply(clean_text)\n",
    "    \n",
    "    # Extract features\n",
    "    df = extract_features(df)\n",
    "    \n",
    "    # Create labels\n",
    "    df = create_troll_labels(df)\n",
    "    \n",
    "    # Select relevant columns for modeling\n",
    "    features = [\n",
    "        'cleaned_text', 'text_length', 'hashtag_count', 'mention_count',\n",
    "        'uppercase_ratio', 'controversial_keyword_count', 'engagement_score',\n",
    "        'is_verified', 'troll_label'\n",
    "    ]\n",
    "    \n",
    "    return df[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Generated 134 potential troll instances out of 537 total records.\n",
      "Processed data saved to '../data/processed_twitter_data.csv'\n",
      "Train/test splits saved to '../data/train_data.csv' and '../data/test_data.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # File path to your data\n",
    "    file_path = 'data/data.csv'\n",
    "    \n",
    "    # Load and process data\n",
    "    df = load_data(file_path)\n",
    "    processed_df = prepare_dataset(df)\n",
    "    \n",
    "    # Save the processed data\n",
    "    processed_df.to_csv('data/processed_twitter_data.csv', index=False)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X = processed_df.drop('troll_label', axis=1)\n",
    "    y = processed_df['troll_label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save train and test sets\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv('data/train_data.csv', index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv('data/test_data.csv', index=False)\n",
    "    \n",
    "    print(f\"Processing complete. Generated {sum(y)} potential troll instances out of {len(y)} total records.\")\n",
    "    print(f\"Processed data saved to '../data/processed_twitter_data.csv'\")\n",
    "    print(f\"Train/test splits saved to '../data/train_data.csv' and '../data/test_data.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_labeling_sample(df, sample_size=200):\n",
    "    \"\"\"Create a sample for manual labeling\"\"\"\n",
    "    # Sample tweets for manual labeling\n",
    "    sampled_df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Add columns for manual labeling\n",
    "    sampled_df['is_troll'] = None\n",
    "    sampled_df['troll_category'] = None\n",
    "    sampled_df['notes'] = None\n",
    "    \n",
    "    # Save the sample for manual labeling\n",
    "    sampled_df.to_csv('../data/manual_labeling_sample.csv', index=False)\n",
    "    \n",
    "    return sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_labeling_sample(df, sample_size=200):\n",
    "    \"\"\"Create a sample for manual labeling\"\"\"\n",
    "    # Sample tweets for manual labeling\n",
    "    sampled_df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Add columns for manual labeling\n",
    "    sampled_df['is_troll'] = None\n",
    "    sampled_df['troll_category'] = None\n",
    "    sampled_df['notes'] = None\n",
    "    \n",
    "    # Save the sample for manual labeling\n",
    "    sampled_df.to_csv('../data/manual_labeling_sample.csv', index=False)\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_sequence_model(max_words=10000, max_sequence_length=100):\n",
    "    \"\"\"Build a sequential model for text classification\"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length),\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m build_text_sequence_model()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m(\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(), tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRecall()]\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Return the model (if needed)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_text_sequence_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Return the model (if needed)\n",
    "model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
